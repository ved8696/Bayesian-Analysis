---
title: "ABDA (week 4) Basics of MCMC: Exercises"
author: "Vedant ShaH 987144"
date: "05/12/2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = F, message = FALSE, warning = FALSE, error = FALSE, fig.width = 5, fig.align = "center")

```

```{r libraries, message = FALSE, warning = FALSE, include = FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for Bayesian regression modeling
library(brms)

# package for visualization
library(tidybayes)

# package to visualize 
library(bayesplot)

# these options help Stan run faster
options(mc.cores = parallel::detectCores())

#devtools::install_github("michael-franke/aida-package")
library(aida)

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 

```

# Instructions

- If you need help, take a look at the suggested readings in the lecture, make use of the Forum, make use of the Forum, and also make use of the Forum.
- Use the file `04-exercises.Rmd`, solve the exercises marked as homework, and save the file with your student number and name.
- ‘Knit’ the document to produce a HTML file.  
- Please do not suppress the code in the HTML-Output!
- Create a ZIP archive called “MATRIKELNR_Lastname_Firstname_ABDA_Week4.zip” containing:
  - an R Markdown file “MATRIKELNR_Lastname_Firstname_ABDA_Week4.Rmd”
  - a knitted HTML document “MATRIKELNR_Lastname_Firstname_ABDA_Week4.html”
- Upload the ZIP archive on Stud.IP in the homework folder before the deadline. You may upload as many times as you like before the deadline, only your final submission will count.

# Preliminaries

To use the latest additions to the course package, make sure to update once with manually executing the command:

```{r}
devtools::install_github("michael-franke/aida-package")
```


We are going to set a seed, so that results from (pseudo-)random executions are repeatable. 

```{r}
set.seed(1969)
```


# Motivation and learning goals

The main **learning goals** of this week's practical part are:

- get familiar with the Metropolis Hastings algorithm
- see the importance of fine-tuning parameter for MH (in particular: the proposal function)
- test samples for their representativeness (diagnostics)
  - calculate and interpret the $\hat{R}$ statistic
  - produce and interpret traceplots
  - calculate and understand efficient sample size
- understand how an "intercept-only" regression model estimates the parameters of a Gaussian model

# <span style = "color:darkgreen">Exercise 1 [SELF-PRACTICE]:</span> A hand-coded MH-sampler for a single normal model

We are going to, again, try to estimate the joint posterior of the mean $\mu$ and standard deviation $\sigma$ that could have generated the following fictitious (flower) data:

```{r}
# flower height data (as before)
heights_A <- c(6.94, 11.77, 8.97, 12.2, 8.48, 
               9.29, 13.03, 13.58, 7.63, 11.47, 
               10.24, 8.99, 8.29, 10.01, 9.47, 
               9.92, 6.83, 11.6, 10.29, 10.7, 
               11, 8.68, 11.71, 10.09, 9.7)

```



# <span style = "color:firebrick">Exercise 3 [HOMEWORK]:</span> Compare efficiency to a much better sampler 

Let's use `brms` to obtain samples from the Gaussian model for the flower data in order to learn two things:

1. How to see that the "single Gaussian" model is a special case of linear regression.

2. To see how much more efficient the sampler is that `brms` relies on.

## <span style = "color:firebrick">Exercise 3a [HOMEWORK]:</span> Intercept-only model

Consider $N$ metric measurements in vector $\vec{y}$ and a regression model for the data in $\vec{y}$ which on contains only an intercept parameter. Write down the likelihood function for this regression model and argue (informally, in one short sentence) that this model is equivalent to a single Gaussian model (of the kind we analyzed above). 



$$
\begin{align*}
\text{y} & \sim \text{N}(\theta_1x + \theta_0, \sigma^2)  \\
\end{align*}
$$
```{r}

#We want to see the error of the point from the line (in this case the intercept)
#The error can be seen as a gaussian distribution with error
#from the point being the deviation and mean being 0
```
## <span style = "color:firebrick">Exercise 3b [HOMEWORK]:</span> Running the intercept-only model with `brms`

Run the following code to obtain samples from the intercept-only model in `brms`. Notice the formula syntax `heights_A ~ 1` where we specify the intercept with the number 1. Notice also that we can set parameters about the MCMC sampling process in the call to `brm`, specifying the number of chains, the samples to obtain and the warmup.

```{r, results = 'hide'}
HMC_samples_Gaussian <- brm(
  formula = heights_A ~ 1,
  data = tibble(heights_A),
  chains = 4, 
  iter = 20000, 
  warmup = 5000
)
```

Use the function `plot` to produce a traceplot from the `brms-fit` object `HMC_samples_Gaussian`. Interpret what you see, in particular say whether the picture suggests convergence.

```{r}
plot(HMC_samples_Gaussian)
```

```{r}
# solution: This looks good enough. The four chains seem to jump around the same set of values. 
# so it can be said the the chains are converging around the same set of values.
```
Use `summary` to obtain more information about the `brms-fit` object `HMC_samples_Gaussian`. Find information on the efficient sample size for these samples and comment on the direct comparison with the hand-made MH-algorithm.

```{r}
summary(HMC_samples_Gaussian)
```

```{r}
# solution: The "point estimate" of the R-hat is 1, which means  
# that the error/difference within each chain is almost the same to the error/difference between 
# chains, so that we might conclude that the chains are indeed going around the same set of values or 
# also called as "typical set"

#This is similiar to the hand-made MH-algorithm which also had a mean of around 10.036 and sigma of 1.85
```


# <span style = "color:firebrick">Exercise 4 [HOMEWORK]:</span> A hand-coded MH-sampler for a regression model

To apply our knowledge of the mechanics of an MH-sampler, let's apply it to our running example of a linear regression of Area-Under-Curve predicted by Maximum-Absolute-Deviation for the mouse-tracking data. Here's the data we'd like to use (as before):

```{r}
# pick up data set
dolphin <- aida::aidata

# aggregate (as before)
dolphin_agg <- dolphin %>% 
  filter(correct == 1) %>% 
  group_by(subject_id) %>% 
  dplyr::summarize(AUC = median(AUC, na.rm = TRUE),
            MAD = median(MAD, na.rm = TRUE)) 
  
# to-be-explained variable y
y <- dolphin_agg$AUC

# explanatory variables x
x <- dolphin_agg$MAD

# number of observations
N <- length(y)

```


## <span style = "color:firebrick">Exercise 4a [HOMEWORK]:</span> Fix the target function

As in the previous case of a Gaussian model, we first define the "target function", the logarithm of non-normalized posteriors. The code below contains four occurrences of `XXX` where you should fill in the required code to make this work.

```{r}
# target function to approximate:
#   log of non-normalized posterior: prior times likelihood
target_f <- function(beta_0, beta_1, sigma){
  # sigma cannot be negative
  if (sigma <=0){
    return(-Inf)
  }
  # prior over coefficients: normal distributions
  prior_beta_0 <- dnorm(beta_0, mean = 0, sd = 10000, log = T)
  prior_beta_1 <- dnorm(beta_1, mean = 0, sd = 10000, log = T)
  # prior over sigma: (truncated) normal distribution (informed by data)
  prior_sigma  <- dnorm(sigma, mean = 15000, sd = 10000, log = T)
  likelihood   <-  sum(map_dbl(1:N, function(i) {dnorm(y[i], mean = beta_0*x[i] + beta_1, sd = sigma , log = T)}))
  return(prior_beta_0+prior_beta_1 + prior_sigma + likelihood)
}

```


## <span style = "color:firebrick">Exercise 4b [HOMEWORK]:</span> Fix the MH-sampler

Here is a version of the MH-sampler with some of the most important parameters that influence sampling success pulled out and initialized very badly. Find values for these parameters such that $\hat{R}$ is below 1.1 for all parameters (or as close to 1.1 as you can get it). Do this for samples of 15,000 iterations per chain (after warm-up), with four chains and a warm-up of 5,000.  

**Hints:** Notice that one sampling round takes quite some time. That's why you really want to understand what changing different parameters will do to the chains. Inspect the traceplots to see what may have gone wrong. Try out a few parameter values, then think. Trial-and-error without understanding is not efficient here.




```{r}

initial_mean_beta_0 <- 500
initial_mean_beta_1 <- 500

initial_SD_beta_0 <- 100
initial_SD_beta_1 <- 100
  
proposal_SD_beta_0 <- 50
proposal_SD_beta_1 <- 1000
  
MH_regression <- function(iterations = 50, chains = 2, warmup = 0){
  # prepare 'container' to store the output
  out <- array(0, dim = c(chains, iterations - warmup, 3))
  dimnames(out) <- list("chain" = 1:chains, 
                       "iteration" = 1:(iterations-warmup), 
                       "variable" = c("beta_0", "beta_1", "sigma"))
  # iterate over chains
  for (c in 1:chains){
    # sample initial parameters in a way that is informed by the data (to ease inference)
    beta_0 <- rnorm(1, mean = initial_mean_beta_0, sd = initial_SD_beta_0)
    beta_1 <- rnorm(1, mean = initial_mean_beta_1, sd = initial_SD_beta_1)
    sigma  <- rnorm(1, mean = 15000, sd = 2000)
    # iterate MCMC steps
    for (i in 1:iterations){
      # generate proposal for next parameter pair
      beta_0_next <- rnorm(1, mean = beta_0, sd = proposal_SD_beta_0)
      beta_1_next <- rnorm(1, mean = beta_1, sd = proposal_SD_beta_1)
      d <- 1
      sigma_next <- rlnorm(1, mean = log(sigma) - 0.5*d^2, sdlog = d)
      # calculate the accept rate (correcting for asymetric proposals for sigma)
      accept_prob <- min(
        exp(
          target_f(beta_0_next, beta_1_next, sigma_next) - 
            target_f(beta_0, beta_1, sigma) +
            dlnorm(sigma, mean = log(sigma_next) - 0.5*d^2, sdlog = d, log = T) - 
            dlnorm(sigma_next, mean = log(sigma) - 0.5*d^2, sdlog = d, log = T)
        ), 
        1
      )
      # check wether to accept the proposal & update
      if (rbernoulli(1, p = accept_prob)) {
        beta_0 <- beta_0_next
        beta_1 <- beta_1_next
        sigma  <- sigma_next
      }
      # record sample (only after warmup)
      if (i >= warmup){
        out[c,i-warmup,1] <- beta_0
        out[c,i-warmup,2] <- beta_1
        out[c,i-warmup,3] <- sigma
      }
    }
  }
  # return samples as an 'mcmc.list' object (from the 'coda' package)
  return(coda::mcmc.list(map(1:chains, function(c) coda::mcmc(out[c,,]))))
}

MH_samples_regression <- MH_regression(
  iterations = 15000, 
  chains = 4, 
  warmup = 5000
)

bayesplot::mcmc_trace(MH_samples_regression)

coda::gelman.diag(MH_samples_regression)
```




## <span style = "color:firebrick">Exercise 4c [HOMEWORK]:</span> Plot the posterior samples 

Make a scatter plot of the samples, with samples for $\beta_0$ on the $x$-axis and samples for $\beta_1$ on the $y$-axis. Comment on whether, given model and data, belief in the proposition that the slope is positive seems warranted.

```{r}

MH_samples_regression <- ggmcmc::ggs(MH_samples_regression)



MH_samples_regression  %>%
  mutate(Chain = factor(Chain)) %>% 
  pivot_wider(id_cols = c(1,2), names_from = Parameter, values_from = value) %>%   ggplot(aes(x = beta_0, y = beta_1, fill = Chain)) +
  geom_point(alpha = 0.4, color = "lightblue")
```

```{r}
#Seeing the Graph we can surely say that the model does not give an positive (upward) slope. The graph clearly indicates a negative (downward) slope. 
```

